#!/usr/bin/python3
import configparser
import datetime
import json
import logging
from logging import handlers
import socket
import subprocess
import time
import os

from github import Github
import iso8601
from openlabcmd import zk
import requests
import six


# Simpledns provider
DOMAIN_NAME = 'openlabtesting.org'

CONFIG_LOCAL = {
    'dns_provider_token': "{{ dns_access_token }}",
    'dns_provider_account': {{ dns_account_id }},
{% if use_test_account %}
    'github_repo': "{{ test_repo_name }}",
    'github_user_token': "{{ test_github_token }}",
{% else %}
    'github_repo': 'theopenlab/openlab',
    'github_user_token': "{{ github_token }}",
{% endif %}
{% if use_test_url %}
    'dns_status_domain': 'test-status.openlabtesting.org',
    'dns_log_domain': 'test-logs.openlabtesting.org',
{% else %}
    'dns_status_domain': 'status.openlabtesting.org',
    'dns_log_domain': 'logs.openlabtesting.org',
{% endif %}
     'dns_master_public_ip': "{{ master_zuul_web_ip }}",
     'dns_slave_public_ip': "{{ slave_zuul_web_ip }}",
}

# General constants
SYSTEMCTL_STATUS = 'status'
SYSTEMCTL_RESTART = 'restart'
SYSTEMCTL_STOP = 'stop'
SYSTEMCTL_START = 'start'


class Base(object):
    def __init__(self, zk, cfg_cache):
        self.zk = zk
        self.node = self.zk.get_node(socket.gethostname())
        self.oppo_node = self._get_oppo_node()
        self.cfg_cache = cfg_cache

    def _get_oppo_node(self):
        for zk_node in self.zk.list_nodes():
            if (zk_node.type == self.node.type and
                    zk_node.name != self.node.name):
                return zk_node

    def _is_check_heart_beat_overtime(self, node_obj):
        try:
            over_time = iso8601.parse_date(
                node_obj.heartbeat) + datetime.timedelta(
                seconds=self.cfg_cache.heartbeat_timeout_second)
        except (iso8601.ParseError, TypeError, ValueError):
            # The heartbeat is not formatted, this must be the kp on the node
            # is not finish the first loop. We just return False, as we are in
            # the initializing process.
            return False
        current_time = datetime.datetime.utcnow().replace(tzinfo=iso8601.UTC)
        return current_time > over_time

    def _ping(self, ipaddr):
        cli = ['ping', '-c1', '-w1', ipaddr]
        proc = subprocess.Popen(cli, stdout=subprocess.PIPE,
                                stderr=subprocess.PIPE)
        proc.communicate()

        return proc.returncode == 0

    def _parse_isotime(self, timestr):
        try:
            return iso8601.parse_date(timestr)
        except iso8601.ParseError as e:
            raise ValueError(six.text_type(e))
        except TypeError as e:
            raise ValueError(six.text_type(e))

    def _is_alarmed_timeout(self, obj):
        over_time = self._parse_isotime(
            obj.alarmed_at) + datetime.timedelta(
            hours=self.cfg_cache.unnecessary_service_switch_timeout_hour)
        current_time = datetime.datetime.utcnow().replace(tzinfo=iso8601.UTC)
        return current_time > over_time

    def _get_service_status(self, service):
        # timer tasks are handled by crontab
        if service in ['zuul-timer-tasks', 'nodepool-timer-tasks']:
            service = 'cron'

        cmd = "systemctl status {srvc}".format(srvc=service)
        try:
            # 0 means OK
            # if using status command, 0 means active, non-zero means error
            # status.
            subprocess.check_output(cmd.split(), stderr=subprocess.STDOUT)
            self.cfg_cache.LOG.debug("Service %(name)s runs well.",
                                     {'name': service})
            return 'up'
        except subprocess.CalledProcessError as e:
            self.cfg_cache.LOG.error("Service %(name)s runs error: "
                                     "%(err)s %(out)s.",
                                     {'name': service,
                                      'err': e,
                                      'out': e.output})
            return 'down'


class Refresher(Base):
    def __init__(self, zk, cfg_cache):
        super(Refresher, self).__init__(zk, cfg_cache)

    def _local_node_service_process(self, node_obj):
        service_objs = self.zk.list_services(node_name_filter=node_obj.name)
        for service_obj in service_objs:
            self._refresh_service(service_obj, node_obj)

        self._report_heart_beat(node_obj)

    def _refresh_service(self, service_obj, node_obj):
        cur_status = self._get_service_status(service_obj.name)
        update_dict = {}
        if cur_status == 'up':
            if service_obj.status != 'up':
                update_dict['status'] = 'up'
                update_dict['restarted'] = False
                update_dict['alarmed'] = False
                self.cfg_cache.LOG.debug("Fix Service %(name)s status from "
                                         "%(orig)s to UP.",
                                         {'name': service_obj.name,
                                          'orig': service_obj.status})
        else:
            if not service_obj.restarted:
                update_dict['status'] = 'restarting'
                update_dict['restarted'] = True
                self.cfg_cache.LOG.debug("Service %(name)s is Restarting.",
                                         {'name': service_obj.name})
            else:
                update_dict['status'] = 'down'
                self.cfg_cache.LOG.debug("Service %(name)s is Down.",
                                         {'name': service_obj.name})
        if update_dict:
            self.zk.update_service(service_obj.name, node_obj.name,
                                   **update_dict)

    def _need_fix_alarmed_status(self, node):
        service_objs = self.zk.list_services(node_name_filter=node.name)
        err_svcs = []
        for service_obj in service_objs:
            if service_obj.status == 'down':
                err_svcs.append(service_obj)

        need_fix = False
        if len(err_svcs) == 0:
            need_fix = True
        else:
            fix_res = []
            for err_svc in err_svcs:
                if (not err_svc.is_necessary and
                        not self._is_alarmed_timeout(err_svc) and
                        node.role == 'master'):
                    fix_res.append(True)
            if len(err_svcs) == len(fix_res):
                need_fix = True

        return need_fix and node.alarmed

    def _report_heart_beat(self, node_obj):
        hb = datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')
        update_dict = {'heartbeat': hb}
        if node_obj.status == 'initializing' or (
                node_obj.role == 'slave' and node_obj.status == 'down'):
            update_dict['status'] = 'up'
        if ((node_obj.alarmed and node_obj.role == 'slave') or
                self._need_fix_alarmed_status(node_obj)):
            update_dict['alarmed'] = False
        self.zk.update_node(node_obj.name, **update_dict)
        self.cfg_cache.LOG.debug("Report node %(name)s heartbeat %(hb)s",
                                 {'name': node_obj.name, 'hb': hb})

    def _oppo_node_check(self, oppo_node_obj):
        if oppo_node_obj.status == 'maintaining':
            return
        if (not self._ping(oppo_node_obj.ip) and
                self._is_check_heart_beat_overtime(oppo_node_obj)):
            if oppo_node_obj.status == 'up':
                self.zk.update_node(oppo_node_obj.name, status='down')
                self.cfg_cache.LOG.info("OPPO %(role)s node %(name)s can not "
                                        "reach, updated with %(status)s status.",
                                        {'role': oppo_node_obj.role,
                                         'name': oppo_node_obj.name,
                                         'status': 'down'.upper()})

    def run(self):
        if self.node.status in ['maintaining', 'down']:
            self.cfg_cache.LOG.debug(
                'Node %(name)s status is %(status)s, Skipping refresh.',
                {'name': self.node.name,
                 'status': self.node.status.upper()})
            return
        self._local_node_service_process(self.node)
        if self.oppo_node:
            self._oppo_node_check(self.oppo_node)


class Fixer(Base):
    def __init__(self, zk, cfg_cache):
        super(Fixer, self).__init__(zk, cfg_cache)

    def _post_alarmed(self, obj):
        if not obj.alarmed:
            if 'node' in obj.__class__.__name__.lower():
                self.zk.update_node(obj.name, alarmed=True)
                self.cfg_cache.LOG.info("%(role)s Node %(name)s updated with "
                                        "alarmed=True",
                                        {'name': obj.name,
                                         'role': obj.role})
            elif 'service' in obj.__class__.__name__.lower():
                self.zk.update_service(obj.name, self.node.name, alarmed=True)
                self.cfg_cache.LOG.info("Service %(name)s updated with "
                                        "alarmed=True",
                                        {'name': obj.name})

    def _service_restart(self, service):
        cmd = "systemctl restart {srvc}".format(srvc=service)
        try:
            # 0 means OK
            # if using status command, 0 means active, non-zero means error status.
            subprocess.check_output(cmd.split(), stderr=subprocess.STDOUT)
            self.cfg_cache.LOG.info("Service %(name)s restarted success.",
                                    {'name': service})
        except subprocess.CalledProcessError as e:
            self.cfg_cache.LOG.error("Service %(name)s restarted failed.: "
                                     "%(err)s %(out)s", {'name': service,
                                                         'err': e,
                                                         'out': e.output})

    def _fix_service(self, service_obj):
        if service_obj.status == 'restarting':
            if service_obj.name in ['zuul-timer-tasks','nodepool-timer-tasks']:
                service_name = 'cron'
            else:
                service_name = service_obj.name
            self._service_restart(service_name)
        elif service_obj.status == 'down':
            if not service_obj.alarmed:
                if service_obj.is_necessary:
                    self._notify_issue(self.node, self.node,
                                       affect_services=service_obj,
                                       affect_range='node',
                                       more_specific='necessary_error')
                    self._post_alarmed(self.node)

                else:
                    self._notify_issue(self.node, self.node,
                                       affect_services=service_obj,
                                       affect_range='service',
                                       more_specific='unecessary_svc')
                self.zk.update_service(service_obj.name, self.node.name,
                                       alarmed=True)
                self._post_alarmed(service_obj)
            else:
                if not service_obj.is_necessary and self._is_alarmed_timeout(
                        service_obj):
                    if self.node.role == 'master' and not self.node.alarmed:
                        self._notify_issue(self.node, self.node,
                                           affect_range='node',
                                           more_specific='slave_switch')
                        self._post_alarmed(self.node)

    def _local_node_service_process(self):
        service_objs = self.zk.list_services(node_name_filter=self.node.name)
        for service_obj in service_objs:
            self._fix_service(service_obj)

    def _format_body_for_issue(self, issuer_node, node_obj,
                               affect_services=None,
                               affect_range=None, more_specific=None):
        title = "[OPENLAB HA][%s] "
        body = "Issuer Host Info:\n" \
               "===============\n" \
               "  name: %(name)s\n" \
               "  role: %(role)s\n" \
               "  ip: %(ip)s\n" % {
                   "name": issuer_node.name,
                   "role": issuer_node.role,
                   "ip": issuer_node.ip
               }

        body += "\nReason:\n" \
                "===============\n"
        if affect_range == 'node':
            if more_specific == 'oppo_check':
                body += "The target node %(name)s in %(role)s deployment is " \
                        "failed to be accessed with IP %(ip)s or fetching its " \
                        "heartbeat.\n" % (
                            {'name': node_obj.name,
                             'role': node_obj.role,
                             'ip': node_obj.ip})
                title += "%s check %s failed, " % (
                    issuer_node.role, node_obj.role)
                if node_obj.role == 'master':
                    body += "HA tools already switch to slave deployment, " \
                            "please try a simple job to check whether " \
                            "everything is OK.\n"
                    title += "HA Switch Going."
                else:
                    title += "need to recover manually."
                body += "\nSuggestion:\n" \
                        "===============\n" \
                        "ssh ubuntu@%s\n" % node_obj.ip
                body += "And try to login the cloud to check whether the " \
                        "resource exists.\n"
            elif more_specific == 'slave_error':
                body += "The data plane services of node %s in slave " \
                        "deployment hit errors.\n" % node_obj.name
                svcs = []
                for svc in node_obj.services:
                    svcs.append(svc.name)
                body += "The affected services including:\n %s\n" % " ".join(
                    svcs)
                body += "\nSuggestion:\n" \
                        "===============\n" \
                        "ssh ubuntu@%s\n" % node_obj.ip
                for s in svcs:
                    body += "systemctl %s %s\n" % (SYSTEMCTL_RESTART, s)
                title += "slave node %s has down, need to recover manually." % (
                    node_obj.name)
            elif more_specific in ['slave_switch', 'necessary_error']:
                body += "HA tools already switch to slave deployment, please " \
                        "try a simple job to check whether everything is OK.\n"
                body += "Currently, the slave deployment has changed to master.\n"
                body += "Please check original master deployment " \
                        "whether it is good for recovery or use labkeeper to " \
                        "re-create an new slave deployment.\n"
                if more_specific == 'necessary_error':
                    body += "This switch happened by necessary service check " \
                            "error.\n"
                    body += "Error service:\n"
                    body += "%s\n" % affect_services.name
                    title += "%s node - %s 's necessary services %s are dead. HA " \
                             "switch going." % (node_obj.role, node_obj.name,
                                                affect_services.name)
                else:
                    title += "%s node - %s is dead. HA " \
                             "switch going." % (node_obj.role, node_obj.name)
                body += "\nSuggestion:\n" \
                        "===============\n" \
                        "ssh ubuntu@%s\n" \
                        "cd go-to-labkeeper-directory\n" \
                        "./deploy.sh -d new-slave\n" % node_obj.ip
            elif more_specific == 'healthchecker_error':
                title += "%s node %s - ha_healthchecker are dead." % (
                    node_obj.role, node_obj.name)
                body += "ha_healthchecker not working anymore, please " \
                        "login to fix it.\n"
                body += "\nSuggestion:\n" \
                        "===============\n" \
                        "ssh ubuntu@%s\n" \
                        "systemctl status ha_healthchecker.timer\n" \
                        "systemctl status ha_healthchecker.service\n" \
                        "systemctl enable ha_healthchecker.service\n" \
                        "systemctl enable ha_healthchecker.timer\n" \
                        "systemctl start ha_healthchecker.timer\n" \
                        "systemctl list-timers --all\n" % node_obj.ip
        elif affect_range == 'service':
            if more_specific == 'unecessary_svc':
                body += "A unecessary serivce %(service_name)s on the node " \
                        "%(name)s (IP %(ip)s) has done. Please go ahead to " \
                        "check.\n" % (
                        {'service_name': affect_services.name,
                         'name': node_obj.name,
                         'ip': node_obj.ip})
                body += "\nSuggestion:\n" \
                        "===============\n" \
                        "ssh ubuntu@%s\n" \
                        "systemctl %s %s\n" \
                        "journalctl -u %s\n" % (
                        node_obj.ip, SYSTEMCTL_STATUS,
                        affect_services.name,
                        affect_services.name)
                title += "%s node - %s 's unnecessary services %s are dead, " \
                         "need to recover manually." % (
                             node_obj.role, node_obj.name,
                             affect_services.name)
        return title, body

    def _notify_issue(self, issuer_node, affect_node, affect_services=None,
                     affect_range=None, more_specific=None):
        title, body = self._format_body_for_issue(
            issuer_node, affect_node, affect_services=affect_services,
            affect_range=affect_range, more_specific=more_specific)
        g = Github(login_or_token=self.cfg_cache.github_user_token)
        repo = g.get_repo(self.cfg_cache.github_repo)
        repo.create_issue(
            title=title % (
                datetime.datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")),
            body=body)
        self.cfg_cache.LOG.info("Post a Issue for %(title)s with reason "
                                "%(reason)s.",
                                {'title': title % (
                                    datetime.datetime.utcnow().strftime(
                                        "%Y-%m-%d %H:%M:%S")),
                                 'reason': more_specific})

    def _oppo_node_check(self, oppo_node_obj):
        if (self._ping(oppo_node_obj.ip) and
                self._is_check_heart_beat_overtime(oppo_node_obj) and
                oppo_node_obj.status != 'down'):
            if not oppo_node_obj.alarmed:
                self._notify_issue(self.node, oppo_node_obj,
                                   affect_range='node',
                                   more_specific='healthchecker_error')
                self._post_alarmed(oppo_node_obj)
        elif (not self._ping(oppo_node_obj.ip) and
              self._is_check_heart_beat_overtime(oppo_node_obj)):
            if oppo_node_obj.status == 'down':
                if not oppo_node_obj.alarmed:
                    self._notify_issue(self.node, oppo_node_obj,
                                       affect_range='node',
                                       more_specific='oppo_check')
                    self._post_alarmed(oppo_node_obj)

    def run(self):
        if self.node.status in ['maintaining', 'down']:
            self.cfg_cache.LOG.debug('Node %(name)s status is %(status)s, '
                                     'Skipping fix.',
                                     {'name': self.node.name,
                                      'status': self.node.status.upper()})
            return
        self._local_node_service_process()
        if self.oppo_node:
            self._oppo_node_check(self.oppo_node)


class Switcher(Base):
    def __init__(self, zk, cfg_cache):
        super(Switcher, self).__init__(zk, cfg_cache)

    def _is_need_switch(self):
        all_nodes = self.zk.list_nodes()

        for node in all_nodes:
            if node.status == 'maintaining':
                return
            elif node.role == 'slave' and node.status == 'down':
                self.cfg_cache.LOG.info("Global checking: there is a slave "
                                        "node %(name)s in DOWN state, can "
                                        "not do switch. Skipping..",
                                        {'name': node.name})
                return

        need_switch = False
        for node in all_nodes:
            if node.role == 'master':
                node_services = self.zk.list_services(
                    node_name_filter=node.name)
                err_services = [e for e in node_services if e.status == 'down']
                if node.status == 'down':
                    need_switch = True
                    self.cfg_cache.LOG.info("Global checking: Found %(role)s "
                                            "node %(name)s is in %(status)s. ",
                                            {'name': node.name,
                                             'role': node.role,
                                             'status': 'down'.upper()})
                    break
                # Service analysis
                for err_svc in err_services:
                    if err_svc.is_necessary:
                        need_switch = True
                        self.cfg_cache.LOG.info(
                            "Global checking: Found a necessary service "
                            "%(service_name)s is in %(service_status)s "
                            "status on %(role)s node %(name)s. ", {
                                'service_name': err_svc.name,
                                'service_status': err_svc.status.upper(),
                                'name': node.name, 'role': node.role})
                        break
                    elif (not err_svc.is_necessary and
                          self._is_alarmed_timeout(err_svc)):
                        need_switch = True
                        self.cfg_cache.LOG.info(
                            "Global checking: Found a necessary service "
                            "%(service_name)s is in %(service_status)s "
                            "status on %(role)s node %(name)s. ", {
                                'service_name': err_svc.name,
                                'service_status': err_svc.status.upper(),
                                'name': node.name, 'role': node.role})
                        break

        if need_switch and not self.node.switch_status:
            self.zk.update_node(self.node.name, switch_status='start')
            self.cfg_cache.LOG.info("Global checking result: setting "
                                    "switch_status %(status)s.",
                                    {'status': 'start'.upper()})

        if self.node.role == 'slave' and (
                self.node.switch_status == 'start' and
                (not self._ping(self.oppo_node.ip) and
                 self._is_check_heart_beat_overtime(self.oppo_node))):
            self.zk.update_node(self.oppo_node.name,
                                switch_status='start')
            self.cfg_cache.LOG.info(
                "Global checking result: setting switch_status "
                "%(status)s and role=slave on OPPO %(role)s "
                "node %(name)s.",
                {'status': 'start'.upper(),
                 'role': self.oppo_node.role,
                 'name': self.oppo_node.name})
        if self.node.role == 'master' and (
                self.node.switch_status == 'end' and
                (not self._ping(self.oppo_node.ip) and
                 self._is_check_heart_beat_overtime(self.oppo_node))):
            self.zk.update_node(self.oppo_node.name, role='slave',
                                switch_status='end')
            self.cfg_cache.LOG.info(
                "Global checking result: setting switch_status "
                "%(status)s and role=slave on OPPO %(role)s "
                "node %(name)s.",
                {'status': 'end'.upper(),
                 'role': self.oppo_node.role,
                 'name': self.oppo_node.name})

        # if we arrive here, that means the switch condition can not reach, but
        # the swtich_status already be set with 'start', that means during the
        # switch preparation period, we fix the switch condition, so this time,
        # we didn't find a switching condition, we must set back the
        # switch_status to None.
        if (not need_switch and self.node.switch_status == 'start' and
                not self._can_start_switch()):
            self.zk.update_node(self.node.name, switch_status=None)
            self.cfg_cache.LOG.info(
                "Global checking result: setting back switch_status "
                "from %(status)s to None on %(role)s node %(name)s.",
                {'status': 'start'.upper(),
                 'role': self.node.role,
                 'name': self.node.name})
        return need_switch

    def _run_systemctl_command(self, command, service):
        cmd = "systemctl {cmd} {srvc}".format(cmd=command, srvc=service)
        try:
            # 0 means OK
            # if using status command, 0 means active, non-zero means error status.
            subprocess.check_output(cmd.split(), stderr=subprocess.STDOUT)
            self.cfg_cache.LOG.debug("Run CMD: %(cmd)s", {'cmd': cmd})
            if command == SYSTEMCTL_STATUS:
                return 'up'
        except subprocess.CalledProcessError as e:
            self.cfg_cache.LOG.error("Failed to %(cmd)s %(srvc)s service: "
                                     "%(err)s %(out)s",
                                     {'cmd': command, 'srvc': service,
                                      'err': e, 'out': e.output})
            if command == SYSTEMCTL_STATUS:
                return 'down'

    def _shut_down_all_services(self, node_obj):
        service_objs = self.zk.list_services(
            node_name_filter=node_obj.name)
        for service in service_objs:
            if service.name not in ['zuul-timer-tasks',
                                    'nodepool-timer-tasks']:
                self._run_systemctl_command(SYSTEMCTL_STOP, service.name)

    def _setup_necessary_services_and_check(self, node_obj):
        service_objs = self.zk.list_services(
            node_name_filter=node_obj.name)
        for service in service_objs:
            if service.is_necessary:
                if service.name not in ['zuul-timer-tasks',
                                        'nodepool-timer-tasks']:
                    self._run_systemctl_command(SYSTEMCTL_START, service)
                    self.cfg_cache.LOG.info("Start Service %(name)s.",
                                            {'name': service.name})

        # local deplay 5 seconds
        time.sleep(5)

        # Then we check all services status in 'SLAVE' env.
        result = {}
        for service in service_objs:
            result[service.name] = self._get_service_status(service.name)

        for svc_name, res in result.items():
            if res != 0:
                self.cfg_cache.LOG.error(
                    "%s is failed to start with return code %s" % (
                        svc_name, res))

    def _match_record(self, name, res):
        return (name == res['name'] and
                res['type'] == "A" and
                self.cfg_cache.dns_master_public_ip == res['content'])

    def _change_dns(self):
        headers = {'Authorization': "Bearer %s" % self.cfg_cache.dns_provider_token,
                   'Accept': 'application/json'}
        res = requests.get(self.cfg_cache.dns_provider_api_url + 'accounts',
                           headers=headers)
        if res.status_code != 200:
            self.cfg_cache.LOG.error("Failed to get the accounts")
            self.cfg_cache.LOG.error(
                "Details: code-status %s\n         message: %s" % (
                    res.status_code, res.reason))
            return
        accounts = json.loads(s=res.content.decode('utf8'))['data']
        account_id = None
        for account in accounts:
            if account['id'] == self.cfg_cache.dns_provider_account:
                account_id = account['id']
                break
        if not account_id:
            self.cfg_cache.LOG.error("Failed to get the account_id")
            return

        target_dict = {self.cfg_cache.dns_status_domain: {},
                       self.cfg_cache.dns_log_domain: {}}
        for target_domain in target_dict.keys():
            res = requests.get(
                self.cfg_cache.dns_provider_api_url + "%s/zones/%s/records?name=%s" % (
                account_id, DOMAIN_NAME,
                target_domain.split(DOMAIN_NAME)[0][:-1]),
                               headers=headers)
            if res.status_code != 200:
                self.cfg_cache.LOG.error(
                    "Failed to get the records by name %s" % target_domain)
                self.cfg_cache.LOG.error(
                    "Details: code-status %s\n         message: %s" % (
                        res.status_code, res.reason))
                return
            records = json.loads(s=res.content.decode('utf8'))['data']
            record_id = None
            for record in records:
                if self._match_record(target_domain.split(DOMAIN_NAME)[0][:-1],
                                record):
                    record_id = record['id']
                    target_dict[target_domain]['id'] = record_id
                    break
            if not record_id:
                self.cfg_cache.LOG.error(
                    "Failed to get the record_id by name %s" % target_domain)
                return

        if not any(target_dict.values()):
            self.cfg_cache.LOG.error("Can't not get any records.")
            return

        headers['Content-Type'] = 'application/json'
        data = {
            "content": self.cfg_cache.dns_slave_public_ip
        }
        for target_domain in target_dict.keys():
            res = requests.patch(
                self.cfg_cache.dns_provider_api_url + "%s/zones/%s/records/%s" % (
                    account_id, DOMAIN_NAME, target_dict[target_domain]['id']),
                data=data, headers=headers)
            result = json.loads(s=res.content.decode('utf8'))['data']
            if (res.status_code == 200 and
                    result['content'] == self.cfg_cache.dns_slave_public_ip):
                self.cfg_cache.LOG.info(
                    "Success Update -- Domain %s from %s to %s" % (
                        target_domain, self.cfg_cache.dns_master_public_ip,
                        self.cfg_cache.dns_slave_public_ip))
            else:
                self.cfg_cache.LOG.error(
                    "Fail Update -- Domain %s from %s to %s" % (
                        target_domain, self.cfg_cache.dns_master_public_ip,
                        self.cfg_cache.dns_slave_public_ip))
                self.cfg_cache.LOG.error(
                    "Details: code-status %s\n         message: %s" % (
                        res.status_code, res.reason))
                return
        self.cfg_cache.LOG.info("Finish update DNS entry.")

    def _can_start_switch(self):
        res = []
        for zk_node in self.zk.list_nodes():
            if (zk_node.role != 'zookeeper' and
                    zk_node.switch_status == 'start'):
                res.append(zk_node.switch_status)
        return len(set(res)) == 1 and len(res) == 4

    def _is_switching(self):
        res = []
        for zk_node in self.zk.list_nodes():
            if zk_node.role != 'zookeeper':
                res.append(zk_node.switch_status)
        return (len(set(res)) == 2 and
                'start' in set(res) and
                'end' in set(res))

    def _do_switch(self, force_switch=False):
        if self.node.role == 'master':
            self._shut_down_all_services(self.node)
            update_dict = {'role': 'slave', 'switch_status': 'end'}
            if not force_switch:
                update_dict['status'] = 'down'
            self.zk.update_node(self.node.name, **update_dict)
            self.cfg_cache.LOG.info(
                "M/S switching: local node, %(role)s node %(name)s is "
                "finishd from master to slave. And update it with "
                "role=slave%(ext_msg)s.",
                {'role': self.node.role, 'name': self.node.name,
                 'ext_msg': ' and status=down' if not force_switch else ''})

        elif self.node.role == 'slave':
            if self.node.type == 'zuul':
                self._change_dns()
            self.zk.update_node(self.node.name, role='master',
                                switch_status='end')
            self._setup_necessary_services_and_check(self.node)
            self.cfg_cache.LOG.info(
                "M/S switching: local node, %(role)s node %(name)s is "
                "finishd from slave to master. And update it with "
                "role=master and switch_status=end.",
                {'role': self.node.role, 'name': self.node.name})

    def _is_end(self):
        res = []
        for zk_node in self.zk.list_nodes():
            if zk_node.role != 'zookeeper':
                res.append(zk_node.switch_status)
        return 'start' not in res

    def run(self):
        is_force_swtich = False
        # This is the normal case, once reach the switch condition we do the
        # normal switch.
        if self._is_need_switch():
            if self._can_start_switch():
                self._do_switch(force_switch=is_force_swtich)
        # But if we found the current switch start are all start, and not reach
        # the switch condition, that means the deploy manager setting the
        # switch_status to start manually and want to do a S/W swtich.
        else:
            if self._can_start_switch():
                is_force_swtich = True
                self._do_switch(force_switch=is_force_swtich)
        if self._is_switching() and self.node.switch_status == 'start':
            self._do_switch(force_switch=is_force_swtich)

        if self._is_end():
            self.zk.update_node(self.node.name, switch_status=None)


class ConfigCache(object):
    def __init__(self, zk):
        self._load(**zk.list_configuration())
        self._refresh_zk(zk)

    def _load(self, **kwargs):
        for attr, value in kwargs.items():
            if not hasattr(self, attr):
                if not value:
                    value = CONFIG_LOCAL.get(attr)
                setattr(self, attr, value)

    def _refresh_zk(self, zk):
        nodes_status = [n.status for n in zk.list_nodes()]
        if len(set(nodes_status)) == 1 and nodes_status[0] == 'initializing':
            for key in CONFIG_LOCAL.keys():
                zk.update_configuration(key, CONFIG_LOCAL[key])
                setattr(self, key, CONFIG_LOCAL[key])

    def configuration(self, log_str):
        file_dir = os.path.split(self.logging_path)[0]
        if not os.path.isdir(file_dir):
            os.makedirs(file_dir)
        if not os.path.exists(self.logging_path):
            os.system('touch %s' % self.logging_path)
        if not self.logging_level.upper() in ['DEBUG', 'INFO', 'ERROR']:
            # use the default level
            self.logging_level = 'DEBUG'
        Rthandler = handlers.RotatingFileHandler(
            self.logging_path, maxBytes=10*1024*1024, backupCount=5)
        logging.basicConfig(
            format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',
            datefmt='%H:%M:%S',
            level=getattr(logging, self.logging_level.upper()),
            handlers=[Rthandler])
        self.LOG = logging.getLogger(log_str)

class HealthChecker(object):
    def __init__(self, config_file):
        cfg = configparser.ConfigParser()
        cfg.read(config_file)
        self.zk = zk.ZooKeeper(cfg)

    def run(self):
        self.zk.connect()
        cfg_cache = ConfigCache(self.zk)
        cfg_cache.configuration(self.__class__.__name__)
        Refresher(self.zk, cfg_cache).run()
        Fixer(self.zk, cfg_cache).run()
        Switcher(self.zk, cfg_cache).run()

        self.zk.disconnect()


if __name__ == '__main__':
    # ZK client config file
    conf = "{{ zk_cli_conf }}"

    HealthChecker(conf).run()
